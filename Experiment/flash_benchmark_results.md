# FlashAttention-2 Benchmark Results

**GPU**: RTX 4060 Laptop (8GB VRAM)  
**Settings**: batch_size=1, causal=True  
**Implementation**: Triton forward + backward kernels (dual-kernel backward: separate dK/dV and dQ kernels)

## Results Table

| seq_len | d | dtype | PT fwd (ms) | PT bwd (ms) | PT e2e (ms) | TR fwd (ms) | TR bwd (ms) | TR e2e (ms) | Fwd Speedup | Bwd Speedup | E2E Speedup |
|--------:|---:|------:|------------:|------------:|------------:|------------:|------------:|------------:|------------:|------------:|------------:|
| 128 | 16 | bf16 | 0.0534 | 0.1091 | 0.2048 | 0.0142 | 0.0478 | 0.1175 | 3.8x | 2.3x | 1.7x |
| 128 | 16 | fp32 | 0.1514 | 0.1400 | 0.2356 | 0.0164 | 0.0581 | 0.1220 | 9.2x | 2.4x | 1.9x |
| 128 | 32 | bf16 | 0.0536 | 0.1211 | 0.2247 | 0.0145 | 0.0551 | 0.1255 | 3.7x | 2.2x | 1.8x |
| 128 | 32 | fp32 | 0.2136 | 0.1484 | 0.3354 | 0.0203 | 0.0717 | 0.1282 | 10.5x | 2.1x | 2.6x |
| 128 | 64 | bf16 | 0.0541 | 0.1183 | 0.2062 | 0.0195 | 0.0723 | 0.1479 | 2.8x | 1.6x | 1.4x |
| 128 | 64 | fp32 | 0.1272 | 0.2006 | 0.3565 | 0.0276 | 0.0977 | 0.1683 | 4.6x | 2.1x | 2.1x |
| 128 | 128 | bf16 | 0.0549 | 0.1088 | 0.2798 | OOM | OOM | OOM | — | — | — |
| 128 | 128 | fp32 | 0.0667 | 0.1088 | 0.2449 | 0.0300 | 0.1110 | 0.1841 | 2.2x | 1.0x | 1.3x |
| 256 | 16 | bf16 | 0.0602 | 0.1817 | 0.2729 | 0.0172 | 0.1053 | 0.1304 | 3.5x | 1.7x | 2.1x |
| 256 | 16 | fp32 | 0.1957 | 0.1506 | 0.3316 | 0.0216 | 0.0763 | 0.1456 | 9.1x | 2.0x | 2.3x |
| 256 | 32 | bf16 | 0.0608 | 0.1510 | 0.2780 | 0.0191 | 0.0680 | 0.1337 | 3.2x | 2.2x | 2.1x |
| 256 | 32 | fp32 | 0.1201 | 0.1578 | 0.3665 | 0.0282 | 0.1017 | 0.1671 | 4.3x | 1.6x | 2.2x |
| 256 | 64 | bf16 | 0.0607 | 0.1692 | 0.2077 | 0.0272 | 0.0913 | 0.1516 | 2.2x | 1.9x | 1.4x |
| 256 | 64 | fp32 | 0.0797 | 0.1456 | 0.3292 | 0.0409 | 0.1492 | 0.2195 | 1.9x | 1.0x | 1.5x |
| 256 | 128 | bf16 | 0.0660 | 0.1199 | 0.2352 | OOM | OOM | OOM | — | — | — |
| 256 | 128 | fp32 | 0.0854 | 0.1507 | 0.2553 | 0.0457 | 0.2562 | 0.2387 | 1.9x | 0.6x | 1.1x |
| 512 | 16 | bf16 | 0.0895 | 0.1865 | 0.2309 | 0.0745 | 0.0755 | 0.1392 | 1.2x | 2.5x | 1.7x |
| 512 | 16 | fp32 | 0.1042 | 0.2116 | 0.3119 | 0.0561 | 0.1119 | 0.1791 | 1.9x | 1.9x | 1.7x |
| 512 | 32 | bf16 | 0.0886 | 0.1278 | 0.2921 | 0.0267 | 0.0971 | 0.2190 | 3.3x | 1.3x | 1.3x |
| 512 | 32 | fp32 | 0.1107 | 0.1701 | 0.2875 | 0.0444 | 0.1611 | 0.2815 | 2.5x | 1.1x | 1.0x |
| 512 | 64 | bf16 | 0.0959 | 0.1960 | 0.2577 | 0.0416 | 0.1373 | 0.2046 | 2.3x | 1.4x | 1.3x |
| 512 | 64 | fp32 | 0.1289 | 0.1834 | 0.4225 | 0.0696 | 0.3181 | 0.5070 | 1.9x | 0.6x | 0.8x |
| 512 | 128 | bf16 | 0.1572 | 0.1403 | 0.2764 | OOM | OOM | OOM | — | — | — |
| 512 | 128 | fp32 | 0.1553 | 0.2452 | 0.5944 | 0.1862 | 0.3081 | 0.4039 | 0.8x | 0.8x | 1.5x |
| 1024 | 16 | bf16 | 0.2463 | 0.2024 | 0.4007 | 0.0398 | 0.1114 | 0.1747 | 6.2x | 1.8x | 2.3x |
| 1024 | 16 | fp32 | 0.2546 | 0.3869 | 0.5854 | 0.1057 | 0.1833 | 0.3166 | 2.4x | 2.1x | 1.8x |
| 1024 | 32 | bf16 | 0.1908 | 0.2144 | 0.4152 | 0.0427 | 0.1515 | 0.3024 | 4.5x | 1.4x | 1.4x |
| 1024 | 32 | fp32 | 0.3495 | 0.3341 | 0.7066 | 0.0751 | 0.3551 | 0.3626 | 4.7x | 0.9x | 1.9x |
| 1024 | 64 | bf16 | 0.1970 | 0.2925 | 0.4184 | 0.0703 | 0.2254 | 0.3087 | 2.8x | 1.3x | 1.4x |
| 1024 | 64 | fp32 | 0.3114 | 0.4474 | 0.9614 | 0.1205 | 0.5652 | 0.6125 | 2.6x | 0.8x | 1.6x |
| 1024 | 128 | bf16 | 0.2352 | 0.2984 | 0.5443 | OOM | OOM | OOM | — | — | — |
| 1024 | 128 | fp32 | 0.4731 | 0.6456 | 1.0707 | 0.2607 | 1.0274 | 1.3852 | 1.8x | 0.6x | 0.8x |
| 2048 | 16 | bf16 | 1.2163 | 0.6231 | 1.6254 | 0.0858 | 0.2351 | 0.3377 | 14.2x | 2.7x | 4.8x |
| 2048 | 16 | fp32 | 1.4576 | 1.3564 | 2.5424 | 0.1260 | 0.4128 | 0.6347 | 11.6x | 3.3x | 4.0x |
| 2048 | 32 | bf16 | 1.0865 | 0.5869 | 1.7593 | 0.1594 | 0.4335 | 0.4763 | 6.8x | 1.4x | 3.7x |
| 2048 | 32 | fp32 | 1.1872 | 1.6236 | 2.8237 | 0.2305 | 1.0210 | 1.1673 | 5.2x | 1.6x | 2.4x |
| 2048 | 64 | bf16 | 1.1586 | 0.5820 | 1.7639 | 0.1810 | 0.7331 | 0.9381 | 6.4x | 0.8x | 1.9x |
| 2048 | 64 | fp32 | 1.5478 | 1.5445 | 2.9429 | 0.4363 | 1.5578 | 2.0716 | 3.5x | 1.0x | 1.4x |
| 2048 | 128 | bf16 | 1.2080 | 0.7778 | 1.8770 | OOM | OOM | OOM | — | — | — |
| 2048 | 128 | fp32 | 1.5681 | 1.9826 | 3.6079 | 0.6453 | 2.5187 | 2.9451 | 2.4x | 0.8x | 1.2x |
| 4096 | 16 | bf16 | 3.1648 | 3.4535 | 6.2420 | 0.1556 | 0.5116 | 0.7486 | **20.3x** | **6.8x** | **8.3x** |
| 4096 | 16 | fp32 | 4.3901 | 5.7733 | 10.1612 | 0.3227 | 1.0062 | 1.3266 | **13.6x** | **5.7x** | **7.7x** |
| 4096 | 32 | bf16 | 3.1572 | 3.1897 | 6.0896 | 0.2012 | 0.6916 | 0.9975 | **15.7x** | **4.6x** | **6.1x** |
| 4096 | 32 | fp32 | 4.5391 | 6.5475 | 10.5715 | 0.4556 | 1.8949 | 2.3672 | **10.0x** | **3.5x** | **4.5x** |
| 4096 | 64 | bf16 | 2.8072 | 3.5581 | 5.6262 | 0.3577 | 1.2348 | 1.9971 | **7.8x** | **2.9x** | **2.8x** |
| 4096 | 64 | fp32 | 4.5462 | 7.0797 | 10.9174 | 0.7716 | 3.1283 | 3.5726 | **5.9x** | **2.3x** | **3.1x** |
| 4096 | 128 | bf16 | 3.1252 | 3.5311 | 6.3732 | OOM | OOM | OOM | — | — | — |
| 4096 | 128 | fp32 | 5.1351 | 8.2364 | 12.1600 | 1.3840 | 5.8259 | 7.6462 | **3.7x** | 1.4x | 1.6x |
| 8192 | 16 | bf16 | 9.5089 | 11.8084 | 21.5322 | 0.3594 | 1.0340 | 1.3660 | **26.5x** | **11.4x** | **15.8x** |
| 8192 | 16 | fp32 | 17.0061 | 23.6380 | 41.9625 | 0.5435 | 2.0081 | 2.8558 | **31.3x** | **11.8x** | **14.7x** |
| 8192 | 32 | bf16 | 8.6966 | 12.3972 | 21.8762 | 0.5039 | 1.6607 | 2.0143 | **17.3x** | **7.5x** | **10.9x** |
| 8192 | 32 | fp32 | 16.8484 | 23.6800 | 38.5116 | 0.9566 | 3.9534 | 4.7473 | **17.6x** | **5.9x** | **8.1x** |
| 8192 | 64 | bf16 | 8.9124 | 11.8948 | 20.9403 | 0.8173 | 2.7737 | 3.7572 | **10.9x** | **4.3x** | **5.6x** |
| 8192 | 64 | fp32 | 15.8689 | 25.5160 | 38.5792 | 1.9155 | 6.8735 | 8.7624 | **8.3x** | **3.7x** | **4.4x** |
| 8192 | 128 | bf16 | 8.5960 | 13.0901 | 22.0068 | OOM | OOM | OOM | — | — | — |
| 8192 | 128 | fp32 | 19.0838 | 28.6269 | 47.1562 | 4.0166 | 16.8484 | 19.9501 | **4.8x** | 1.7x | **2.4x** |

## Key Observations

1. **Forward pass speedups are consistently strong** — Triton forward is 2–31x faster, with the best speedups at long sequences and small d.
2. **Backward pass shows improvement at larger sequences** — At seq_len ≥ 4096, Triton backward achieves 2–12x speedup. At smaller sequences, the overhead of two separate kernels can reduce the advantage.
3. **End-to-end (fwd+bwd) speedups scale with sequence length** — Up to **15.8x** at seq=8192, d=16, bf16.
4. **bf16 is generally faster than fp32** for both implementations, with Triton benefiting more from reduced precision.
5. **d=128 with bf16 hits shared memory OOM** in the backward kernel due to the larger tile sizes needed.
6. **Backward kernel is slower than PyTorch for some small-seq configs** (e.g., seq=512, d=64, fp32) — the kernel launch overhead and two separate kernels dominate at small problem sizes.
