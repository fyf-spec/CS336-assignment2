# FlashAttention & Distributed Data Parallel ç¬”è®°

## ç›®å½•

- [Part 1: FlashAttention](#part-1-flashattention)
  - [1.1 æ ‡å‡† Attention çš„ç“¶é¢ˆ](#11-æ ‡å‡†-attention-çš„ç“¶é¢ˆ)
  - [1.2 FlashAttention-2 Forward (PyTorch)](#12-flashattention-2-forward-pytorch)
  - [1.3 FlashAttention-2 Forward (Triton Kernel)](#13-flashattention-2-forward-triton-kernel)
  - [1.4 FlashAttention-2 Backward](#14-flashattention-2-backward)
- [Part 2: Distributed Data Parallel (DDP)](#part-2-distributed-data-parallel-ddp)
  - [2.1 DDP çš„åŸºæœ¬åŸç†](#21-ddp-çš„åŸºæœ¬åŸç†)
  - [2.2 Naive DDP â€” é€å‚æ•°åŒæ­¥](#22-naive-ddp--é€å‚æ•°åŒæ­¥)
  - [2.3 Flat All-Reduce â€” å‡å°‘é€šä¿¡è°ƒç”¨æ¬¡æ•°](#23-flat-all-reduce--å‡å°‘é€šä¿¡è°ƒç”¨æ¬¡æ•°)
  - [2.4 Overlapping â€” é€šä¿¡ä¸è®¡ç®—é‡å ](#24-overlapping--é€šä¿¡ä¸è®¡ç®—é‡å )
  - [2.5 Bucketed DDP â€” ä¸¤å…¨å…¶ç¾](#25-bucketed-ddp--ä¸¤å…¨å…¶ç¾)
  - [2.6 DDP é€šä¿¡å¼€é”€å»ºæ¨¡](#26-ddp-é€šä¿¡å¼€é”€å»ºæ¨¡)
- [Benchmark æ±‡æ€»](#benchmark-æ±‡æ€»)

---

## Part 1: FlashAttention

### 1.1 æ ‡å‡† Attention çš„ç“¶é¢ˆ

æ ‡å‡† self-attention è®¡ç®— $O = \text{softmax}(QK^T / \sqrt{d}) \cdot V$ï¼Œéœ€è¦ **æ˜¾å¼ç‰©åŒ–** $N \times N$ çš„æ³¨æ„åŠ›çŸ©é˜µï¼š

| æ­¥éª¤ | å†…å­˜å ç”¨ | IO æ“ä½œ |
|------|---------|--------|
| $S = QK^T$ | $O(N^2)$ å†™å…¥ HBM | è¯» Q,K â†’ å†™ S |
| $P = \text{softmax}(S)$ | $O(N^2)$ è¯»+å†™ HBM | è¯» S â†’ å†™ P |
| $O = PV$ | è¯» P å’Œ V | è¯» P,V â†’ å†™ O |

**æ ¸å¿ƒé—®é¢˜**ï¼š
1. **å†…å­˜å ç”¨ $O(N^2)$**ï¼šåºåˆ—é•¿åº¦ N=4096 æ—¶ï¼Œæ³¨æ„åŠ›çŸ©é˜µå  64MB/headï¼ˆfp32ï¼‰ï¼Œæ˜¯å†…å­˜ä¸­æœ€å¤§çš„ activation
2. **IO ç“¶é¢ˆ**ï¼šæ³¨æ„åŠ›çŸ©é˜µåå¤åœ¨ HBM å’Œ SRAM ä¹‹é—´æ¬è¿ï¼ŒGPU çš„ç®—åŠ›åˆ©ç”¨ç‡ä½ï¼ˆcompute-bound â†’ memory-boundï¼‰

> [!IMPORTANT]
> ä» memory profiling ä¸­è§‚å¯Ÿåˆ°ï¼šæœ€å¤§çš„å†…å­˜åˆ†é…æ¥è‡ª `torch.bmm(Q, K^T)` äº§ç”Ÿçš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå½¢çŠ¶ä¸º `(batch, num_heads, seq_len, seq_len)`ï¼Œéšåºåˆ—é•¿åº¦**äºŒæ¬¡å¢é•¿**ã€‚

### 1.2 FlashAttention-2 Forward (PyTorch)

**æ ¸å¿ƒæ€æƒ³**ï¼šTiling + Online Softmaxï¼Œ**æ°¸è¿œä¸ç‰©åŒ–å®Œæ•´çš„ $N \times N$ æ³¨æ„åŠ›çŸ©é˜µ**ã€‚

#### ç®—æ³•æµç¨‹

```
å¯¹æ¯ä¸ª Q çš„ tile (å¤§å° B_q):
    åˆå§‹åŒ– O_i = 0, l_i = 0, m_i = -âˆ
    
    å¯¹æ¯ä¸ª K/V çš„ tile (å¤§å° B_k):
        S_i = Q_i @ K_j^T / âˆšd          â† ä»… B_q Ã— B_k å¤§å°çš„ tile
        m_i_new = max(m_i, rowmax(S_i))  â† åœ¨çº¿æ›´æ–° softmax åˆ†æ¯
        P_i = exp(S_i - m_i_new)
        l_i = exp(m_i - m_i_new) * l_i + rowsum(P_i)
        O_i = exp(m_i - m_i_new) * O_i + P_i @ V_j
        m_i = m_i_new
    
    O_i = O_i / l_i                      â† æœ€ç»ˆå½’ä¸€åŒ–
    L_i = m_i + log(l_i)                 â† ä¿å­˜ logsumexp ä¾› backward ç”¨
```

**å…³é”®ç‚¹**ï¼š
- æ¯æ¬¡åªè®¡ç®— $B_q \times B_k$ å¤§å°çš„ $S$ tileï¼Œå†…å­˜ä» $O(N^2) \to O(B_q \cdot B_k)$
- **Online Softmax**ï¼šç”¨ `m_i`ï¼ˆrunning maxï¼‰å’Œ `l_i`ï¼ˆrunning sumï¼‰å¢é‡è®¡ç®— softmaxï¼Œæ— éœ€çœ‹åˆ°å®Œæ•´è¡Œ
- åªä¿å­˜ $O$ å’Œ $L = m + \log(l)$ï¼ˆlogsumexpï¼‰ï¼Œ**ä¸ä¿å­˜ P çŸ©é˜µ**

> [!TIP]
> ä¸ºä»€ä¹ˆèƒ½ä¸ä¿å­˜ Pï¼Ÿå› ä¸º backward æ—¶å¯ä»¥ç”¨ Q, K, L **é‡æ–°è®¡ç®—** P = exp(S - L)ã€‚ç”¨å°‘é‡é‡è®¡ç®—æ¢å–å·¨å¤§çš„å†…å­˜èŠ‚çº¦ï¼Œè¿™å°±æ˜¯ recomputation ç­–ç•¥ã€‚

#### å®ç°ç»†èŠ‚

```python
# flashattention2.py - forward
B_q = max(16, min(N_q, 64))   # tile å¤§å°é€‰æ‹©
B_k = max(16, min(N_k, 64))

# ä¿å­˜ç”¨äº backward çš„ä¿¡æ¯ï¼ˆä¸ä¿å­˜ Pï¼ï¼‰
ctx.save_for_backward(L, Q, K, V, O)
```

### 1.3 FlashAttention-2 Forward (Triton Kernel)

**ä¸ºä»€ä¹ˆè¦ç”¨ Tritonï¼Ÿ** PyTorch è™½ç„¶èƒ½å®ç° FlashAttention çš„ç®—æ³•é€»è¾‘ï¼Œä½†æ— æ³•ç›´æ¥æ§åˆ¶ GPU çš„ SRAM å’Œçº¿ç¨‹è°ƒåº¦ã€‚Triton å¯ä»¥ï¼š

| ç‰¹æ€§ | PyTorch å®ç° | Triton å®ç° |
|------|------------|------------|
| SRAM æ§åˆ¶ | æ— æ³•ç›´æ¥æ§åˆ¶ | `tl.load` æ˜¾å¼ç®¡ç† |
| çº¿ç¨‹å¹¶è¡Œ | ä¾èµ–ç¼–è¯‘å™¨ | æ‰‹åŠ¨è®¾ç½® grid å’Œ tile |
| Kernel èåˆ | å¤šæ¬¡ HBM è¯»å†™ | å•ä¸ª kernel å®Œæˆ |
| å› æœ mask | é¢å¤–çŸ©é˜µæ“ä½œ | ç¼–è¯‘æœŸå¸¸é‡ `tl.constexpr` |

#### Triton Kernel ç»“æ„

```python
@triton.jit
def flash_fwd_kernel(..., is_causal: tl.constexpr, D: tl.constexpr, ...):
    # æ¯ä¸ª program å¤„ç†ä¸€ä¸ª (batch, q_tile)
    batch_idx = tl.program_id(0)
    q_tile_idx = tl.program_id(1)
    
    # åœ¨ SRAM ä¸­å®Œæˆæ‰€æœ‰è®¡ç®—
    for k_tile_idx in range(num_k_tiles):
        # å› æœ mask: ç¼–è¯‘æ—¶ç¡®å®šè·³è¿‡æ¡ä»¶
        if is_causal:
            if k_start > q_end:
                continue  # æ•´ä¸ª tile è¢« maskï¼Œç›´æ¥è·³è¿‡
```

### 1.4 FlashAttention-2 Backward

**æŒ‘æˆ˜**ï¼šæ ‡å‡† backward éœ€è¦ $P$ çŸ©é˜µï¼ˆ$N \times N$ï¼‰ï¼Œä½† forward æ²¡æœ‰ä¿å­˜å®ƒã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š**é‡è®¡ç®— (Recomputation)**ã€‚ä¿å­˜ logsumexp $L$ï¼Œbackward æ—¶é‡å»º $P$ï¼š

$$P = \exp(S - L), \quad S = QK^T / \sqrt{d}$$

#### Backward çš„æ–¹ç¨‹ï¼ˆEquations 13-19ï¼‰

| æ–¹ç¨‹ | å…¬å¼ | å«ä¹‰ |
|------|------|------|
| Eq 13 | $S = QK^T / \sqrt{d}$ | é‡è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° |
| Eq 14 | $P = \exp(S - L)$ | ç”¨ logsumexp é‡å»ºæ¦‚ç‡ |
| Eq 15 | $dV = P^T \cdot dO$ | V çš„æ¢¯åº¦ |
| Eq 16 | $dP = dO \cdot V^T$ | ä¸­é—´æ¢¯åº¦ |
| Eq 17 | $dS = P \odot (dP - D)$ | S çš„æ¢¯åº¦ï¼Œ$D = \text{rowsum}(O \odot dO)$ |
| Eq 18 | $dQ = dS \cdot K / \sqrt{d}$ | Q çš„æ¢¯åº¦ |
| Eq 19 | $dK = dS^T \cdot Q / \sqrt{d}$ | K çš„æ¢¯åº¦ |

#### Triton Backward çš„åŒ Kernel ç­–ç•¥

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª kernelï¼Ÿ** å› ä¸º dK/dV å’Œ dQ çš„å¤–å¾ªç¯ç»´åº¦ä¸åŒï¼š

```
Kernel 1 (flash_bwd_kernel_KV): å¤–å¾ªç¯éå† K/V tiles
    â†’ è‡ªç„¶ç´¯åŠ  dK, dVï¼ˆæ¯ä¸ª K tile æ±‡æ€»æ‰€æœ‰ Q tiles çš„è´¡çŒ®ï¼‰
    â†’ ä½† dQ éœ€è¦åŸå­æ“ä½œï¼ˆå¤šä¸ª K tile åŒæ—¶å†™åŒä¸€ä¸ª dQï¼‰ï¼Œå¼€é”€å¤§

Kernel 2 (flash_bwd_kernel_Q): å¤–å¾ªç¯éå† Q tiles
    â†’ è‡ªç„¶ç´¯åŠ  dQï¼ˆæ¯ä¸ª Q tile æ±‡æ€»æ‰€æœ‰ K tiles çš„è´¡çŒ®ï¼‰
```

è¿™æ¯”å• kernel + åŸå­æ“ä½œæ›´å¿«ï¼Œè™½ç„¶ P çŸ©é˜µè¢«é‡è®¡ç®—äº†ä¸¤æ¬¡ã€‚

---

## Part 2: Distributed Data Parallel (DDP)

### 2.1 DDP çš„åŸºæœ¬åŸç†

**ç›®æ ‡**ï¼šå¤šGPUä¸Šç”¨æ›´å¤§çš„æœ‰æ•ˆ batch size è®­ç»ƒï¼Œæ¯ä¸ª GPU åªå¤„ç† $n/d$ ä¸ªæ ·æœ¬ã€‚

```mermaid
flowchart LR
    A["Batch (n samples)"] --> B1["GPU 0: n/d samples"]
    A --> B2["GPU 1: n/d samples"]
    B1 --> C1["Forward + Backward"]
    B2 --> C2["Forward + Backward"]
    C1 --> D["All-Reduce æ¢¯åº¦"]
    C2 --> D
    D --> E1["GPU 0: Optimizer Step"]
    D --> E2["GPU 1: Optimizer Step"]
```

**å…³é”®ä¿è¯**ï¼šå› ä¸ºåˆå§‹å‚æ•°ç›¸åŒ + æ¢¯åº¦ç›¸åŒï¼ˆall-reduceåï¼‰â†’ optimizer step åå‚æ•°ä»ç„¶ç›¸åŒã€‚

### 2.2 Naive DDP â€” é€å‚æ•°åŒæ­¥

**å®ç°æ–¹å¼**ï¼šbackward å®Œæˆåï¼Œå¯¹æ¯ä¸ªå‚æ•° tensor é€ä¸€è°ƒç”¨ `dist.all_reduce`ã€‚

```python
def finish_gradient_synchronization(self):
    for param in self.module.parameters():
        dist.all_reduce(param.grad, op=ReduceOp.SUM)
        param.grad /= world_size
```

**ä¸¤ä¸ªé—®é¢˜**ï¼š

> [!WARNING]
> 1. **é€šä¿¡è°ƒç”¨æ¬¡æ•°è¿‡å¤š**ï¼š111 ä¸ªå‚æ•° tensor = 111 æ¬¡ all-reduce è°ƒç”¨ï¼Œæ¯æ¬¡è°ƒç”¨éƒ½æœ‰å›ºå®šå¼€é”€ $o$
> 2. **æ— æ³•ä¸è®¡ç®—é‡å **ï¼šå¿…é¡»ç­‰ backward å…¨éƒ¨å®Œæˆæ‰å¼€å§‹é€šä¿¡ï¼Œé€šä¿¡æ—¶é—´ 100% æ˜¯é¢å¤–å¼€é”€

### 2.3 Flat All-Reduce â€” å‡å°‘é€šä¿¡è°ƒç”¨æ¬¡æ•°

**è¦è§£å†³çš„é—®é¢˜**ï¼šæ¯æ¬¡ all-reduce éƒ½æœ‰å›ºå®šçš„ launch overhead $o$ï¼Œ111 æ¬¡è°ƒç”¨ = $111 \times o$ é¢å¤–å¼€é”€ã€‚

**æ–¹æ³•**ï¼šæŠŠæ‰€æœ‰æ¢¯åº¦æ‹¼æˆä¸€ä¸ªå¤§ tensorï¼Œåªåš 1 æ¬¡ all-reduceã€‚

```python
def finish_gradient_synchronization_flat(self):
    flat = _flatten_dense_tensors(all_grads)  # æ‹¼æˆä¸€ä¸ª tensor
    dist.all_reduce(flat)                      # 1 æ¬¡è°ƒç”¨
    # æ‹†å›å„å‚æ•°çš„ grad
```

**æ•ˆæœ**ï¼šé€šä¿¡è°ƒç”¨ä» 111 æ¬¡ â†’ 1 æ¬¡ï¼Œå‡å°‘äº† per-call overheadã€‚

| æ–¹æ³• | é€šä¿¡è°ƒç”¨æ¬¡æ•° | ç¼ºç‚¹ |
|------|------------|------|
| naive | N æ¬¡ | è°ƒç”¨æ¬¡æ•°å¤š |
| flat | 1 æ¬¡ | å¿…é¡»ç­‰æ‰€æœ‰æ¢¯åº¦å°±ç»ªï¼ˆæ— æ³•overlapï¼‰ |

### 2.4 Overlapping â€” é€šä¿¡ä¸è®¡ç®—é‡å 

**è¦è§£å†³çš„é—®é¢˜**ï¼šæ— è®º naive è¿˜æ˜¯ flatï¼Œé€šä¿¡éƒ½å‘ç”Ÿåœ¨ backward **ä¹‹å**ï¼Œæ˜¯çº¯é¢å¤–å¼€é”€ã€‚

**å…³é”®æ´å¯Ÿ**ï¼šBackward æ˜¯é€å±‚è®¡ç®—çš„ã€‚å½“æœ€åä¸€å±‚çš„æ¢¯åº¦ç®—å®Œæ—¶ï¼Œç¬¬ä¸€å±‚è¿˜åœ¨ç®—â€”â€”å¯ä»¥**è¾¹ç®—è¾¹ä¼ **ï¼

**å®ç°**ï¼š`register_post_accumulate_grad_hook` + `async_op=True`

```python
# __init__ ä¸­æ³¨å†Œ hook
param.register_post_accumulate_grad_hook(self._make_hook())

def _make_hook(self):
    def hook(param):
        # æ¢¯åº¦ä¸€å°±ç»ªå°±ç«‹åˆ»å¯åŠ¨ async all-reduce
        handle = dist.all_reduce(param.grad, async_op=True)
        self._handles.append((handle, param))
    return hook

# backward ä¹‹ååªéœ€ç­‰å¾…
def finish_gradient_synchronization(self):
    for handle, param in self._handles:
        handle.wait()           # å¤§éƒ¨åˆ†å·²ç»å®Œæˆäº†ï¼
        param.grad /= world_size
```

**æ—¶é—´çº¿å¯¹æ¯”**ï¼š

```
Naive:       [  Backward  ] [====== All-Reduce ======] [Opt]
                                 â†‘ çº¯å¼€é”€

Overlapping: [  Backward  â†â”€â”€ åŒæ—¶ All-Reduce â”€â”€â†’    ] [wait] [Opt]
                                                        â†‘ åªç­‰æœ€åå‡ ä¸ª
```

### 2.5 Bucketed DDP â€” ä¸¤å…¨å…¶ç¾

**è¦è§£å†³çš„é—®é¢˜**ï¼šOverlapping è§£å†³äº†é‡å é—®é¢˜ï¼Œä½†ä»ç„¶æ¯ä¸ªå‚æ•°ä¸€æ¬¡ all-reduce è°ƒç”¨ï¼ˆ111 æ¬¡ï¼‰ã€‚Flat åªéœ€ 1 æ¬¡ä½†ä¸èƒ½é‡å ã€‚èƒ½å¦**æ—¢å‡å°‘è°ƒç”¨åˆèƒ½é‡å **ï¼Ÿ

**æ–¹æ³•**ï¼šæŠŠå‚æ•°åˆ†æˆè‹¥å¹²**æ¡¶ (bucket)**ï¼Œæ¯ä¸ªæ¡¶æœ€å¤š `bucket_size_mb` å¤§å°ã€‚å½“ä¸€ä¸ªæ¡¶å†…æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦éƒ½å°±ç»ªæ—¶ï¼ŒæŠŠå®ƒä»¬æ‹¼èµ·æ¥åšä¸€æ¬¡ async all-reduceã€‚

```
å‚æ•°ï¼ˆåå‘éå†ï¼‰:  [p111] [p110] [p109] ... [p2] [p1]
                  |â† Bucket 0 â†’|â†  Bucket 1  â†’| ... |â† Bucket N â†’|

Backward:  p111 æ¢¯åº¦å°±ç»ª â†’ p110 å°±ç»ª â†’ bucket 0 æ»¡ â†’ ğŸš€ async all-reduce
           p109 å°±ç»ª â†’ p108 å°±ç»ª â†’ ... â†’ bucket 1 æ»¡ â†’ ğŸš€ async all-reduce
           ...
```

**å®ç°å…³é”®**ï¼š

```python
# æ¯ä¸ªå‚æ•°çš„ hook é€’å‡æ‰€åœ¨æ¡¶çš„ pending è®¡æ•°
def hook(param):
    self._bucket_pending[bucket_idx] -= 1
    if self._bucket_pending[bucket_idx] == 0:
        self._allreduce_bucket(bucket_idx)  # æ¡¶æ»¡äº†å°±å‘å°„ï¼

# æ¡¶å†…æ¢¯åº¦æ‹¼æˆä¸€ä¸ª tensor å† all-reduce
def _allreduce_bucket(self, bucket_idx):
    flat = _flatten_dense_tensors(bucket_grads)
    handle = dist.all_reduce(flat, async_op=True)
```

> [!TIP]
> ä¸ºä»€ä¹ˆæŒ‰ **reverse order** åˆ†æ¡¶ï¼Ÿå› ä¸º backward ä»æœ€åä¸€å±‚å¼€å§‹ç®—æ¢¯åº¦ï¼Œreverse order ä¿è¯åŒä¸€ä¸ªæ¡¶å†…çš„å‚æ•°æ¢¯åº¦**å‡ ä¹åŒæ—¶å°±ç»ª**ï¼Œå‡å°‘ç­‰å¾…ã€‚

### 2.6 DDP é€šä¿¡å¼€é”€å»ºæ¨¡

å‡è®¾ï¼š
- $s$ï¼šæ¨¡å‹å‚æ•°æ€»å¤§å°ï¼ˆbytesï¼‰
- $w$ï¼šall-reduce ç®—æ³•å¸¦å®½ï¼ˆbytes/sï¼‰
- $o$ï¼šæ¯æ¬¡é€šä¿¡è°ƒç”¨çš„å›ºå®šå¼€é”€ï¼ˆsecondsï¼‰
- $n_b$ï¼šæ¡¶çš„æ•°é‡
- æ¯ä¸ªæ¡¶çš„è®¡ç®—æ—¶é—´ â‰ˆ é€šä¿¡æ—¶é—´ï¼ˆé¢˜ç›®å‡è®¾ï¼‰

**DDP overhead**ï¼ˆbackward ä¹‹åçš„é¢å¤–ç­‰å¾…æ—¶é—´ï¼‰ï¼š

$$\text{overhead} = \frac{s}{n_b \cdot w} + o$$

> æœ€åä¸€ä¸ªæ¡¶åœ¨ backward ç»“æŸæ—¶æ‰å¼€å§‹é€šä¿¡ï¼Œéœ€è¦å®Œæ•´çš„ $\frac{s / n_b}{w}$ æ—¶é—´æ¥å®Œæˆ all-reduceï¼Œå†åŠ ä¸Šä¸€æ¬¡è°ƒç”¨å¼€é”€ $o$ã€‚å‰é¢çš„æ¡¶éƒ½å·²ç»åœ¨ backward æœŸé—´å®Œæˆäº†ã€‚

**æœ€ä¼˜æ¡¶å¤§å°**ï¼šä»¤ $\frac{d(\text{overhead})}{d(n_b)} = 0$ï¼š

$$\frac{d}{dn_b}\left(\frac{s}{n_b \cdot w} + o\right) = -\frac{s}{n_b^2 \cdot w} = 0$$

è¿™ä¸ªæ¨¡å‹ä¸‹ overhead å…³äº $n_b$ å•è°ƒé€’å‡ï¼Œä½†å—é™äºæ€»é€šä¿¡æ—¶é—´ä¸èƒ½å°äº $\frac{s}{w}$ï¼Œä¸”æ¯ä¸ªæ¡¶æœ‰å›ºå®šå¼€é”€ $o$ï¼Œæ‰€ä»¥è€ƒè™‘**æ€»é€šä¿¡æ—¶é—´**ï¼š

$$T_{comm} = n_b \cdot o + \frac{s}{w}$$

overlap èƒ½éšè—çš„é€šä¿¡ = $(n_b - 1)$ ä¸ªæ¡¶çš„é€šä¿¡æ—¶é—´ï¼Œæ•…ï¼š

$$\text{overhead} = \frac{s}{n_b \cdot w} + o$$

æœ€ä¼˜ $n_b$ ä½¿å¾—æ¯ä¸ªæ¡¶çš„é€šä¿¡æ—¶é—´ = è®¡ç®—æ—¶é—´ã€‚è®¾ $T_{compute} = T_{backward} / n_b$ï¼š

$$\frac{s}{n_b \cdot w} = \frac{T_{backward}}{n_b} \implies \text{è‡ªåŠ¨æ»¡è¶³}$$

ä½†å½“ $n_b$ è¿‡å¤§æ—¶å¼€é”€ $n_b \cdot o$ å¢åŠ ã€‚å¹³è¡¡ç‚¹ï¼š

$$\text{optimal bucket size} = \sqrt{\frac{s \cdot o}{w}}$$

---

## Benchmark æ±‡æ€»

å®éªŒé…ç½®ï¼šSmall model (768-dim, 12 layers, 12 heads)ï¼Œ128.6M å‚æ•°ï¼Œ2 GPUsï¼Œgloo backendã€‚

### Naive vs Flatï¼ˆæ—  overlapï¼‰

| æ–¹æ³• | Total (ms) | Comm (ms) | Comm% | vs naive |
|------|-----------|-----------|-------|----------|
| naive (111 calls) | 930.3 | 835.0 | 89.8% | 1.00x |
| flat (1 call) | 709.6 | 615.2 | 86.7% | **1.31x** |

> **ç»“è®º**ï¼šä»…å‡å°‘é€šä¿¡è°ƒç”¨æ¬¡æ•°å°±å¸¦æ¥ 31% åŠ é€Ÿï¼Œè¯´æ˜ per-call overhead å¾ˆæ˜¾è‘—ã€‚

### åŠ å…¥ Overlap

| æ–¹æ³• | Total (ms) | Comm/Wait (ms) | Overhead% | vs naive |
|------|-----------|----------------|-----------|----------|
| naive | 930.3 | 835.0 | 89.8% | 1.00x |
| flat | 709.6 | 615.2 | 86.7% | 1.31x |
| overlap-individual | 735.5 | 638.9 | 86.9% | **1.26x** |

> **ç»“è®º**ï¼šåœ¨ gloo backend ä¸‹ overlap æ•ˆæœæœ‰é™ï¼Œå› ä¸º gloo å¤„ç† GPU tensor æ—¶éœ€è¦ GPUâ†’CPUâ†’é€šä¿¡â†’CPUâ†’GPU æ‹·è´ï¼ŒçœŸæ­£çš„ async ç¨‹åº¦ä¸é«˜ã€‚åœ¨ NCCL backend ä¸‹é¢„æœŸ overlap æ•ˆæœæ›´æ˜¾è‘—ã€‚

### Bucketed DDPï¼ˆä¸åŒæ¡¶å¤§å°ï¼‰

| æ–¹æ³• | æ¡¶æ•° | Total (ms) | Wait (ms) | Overhead% | vs naive |
|------|------|-----------|-----------|-----------|----------|
| naive | 111 | 930.3 | 835.0 | 89.8% | 1.00x |
| flat | 1 | 709.6 | 615.2 | 86.7% | 1.31x |
| overlap-indiv | 111 | 735.5 | 638.9 | 86.9% | 1.26x |
| **bucket-1MB** | 111 | 808.7 | 712.1 | 88.0% | 1.15x |
| **bucket-10MB** | 50 | 700.3 | 603.2 | 86.1% | 1.33x |
| **bucket-100MB** | 6 | **673.6** | **573.3** | **85.1%** | **1.38x** |
| **bucket-1000MB** | 1 | 701.7 | 586.0 | 83.5% | 1.33x |

> [!IMPORTANT]
> **æœ€ä¼˜æ¡¶å¤§å° â‰ˆ 100MB**ï¼ˆ6 bucketsï¼‰ï¼Œå®ç°äº† overlap + batching çš„æœ€ä¼˜å¹³è¡¡ã€‚
> - æ¡¶å¤ªå° (1MB â†’ 111 buckets)ï¼šé€€åŒ–ä¸º overlap-individualï¼Œper-call overhead è¿‡å¤š
> - æ¡¶å¤ªå¤§ (1000MB â†’ 1 bucket)ï¼šé€€åŒ–ä¸º flatï¼Œæ— æ³• overlap
> - ä¸­é—´å€¼ (100MB â†’ 6 buckets)ï¼šå…¼é¡¾ä¸¤è€…ä¼˜åŠ¿

### æ–¹æ³•æ¼”è¿›æ€»ç»“

```mermaid
flowchart TD
    A["Naive DDP<br/>111 calls, no overlap<br/>930 ms"] -->|"å‡å°‘è°ƒç”¨æ¬¡æ•°"| B["Flat All-Reduce<br/>1 call, no overlap<br/>710 ms (1.31x)"]
    A -->|"é€šä¿¡ä¸è®¡ç®—é‡å "| C["Overlapping<br/>111 async calls<br/>736 ms (1.26x)"]
    B -->|"åŠ å…¥ overlap"| D["Bucketed DDP<br/>N buckets, overlapped<br/>674 ms (1.38x)"]
    C -->|"å‡å°‘è°ƒç”¨æ¬¡æ•°"| D
    
    style D fill:#2d6,stroke:#333,color:#fff
```
